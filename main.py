# -*- coding: utf-8 -*-
import os
import re
import io
import time
import json
import unicodedata
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment

try:
    import google.generativeai as genai
except Exception:
    genai = None

try:
    import jaconv # pip install jaconv
except Exception:
    jaconv = None

# ===== è¨­å®š =====
RELEASE_TAG = "news-latest"
ASSET_NAME = "yahoo_news.xlsx"
SHEET_NAMES = [
    "ãƒ›ãƒ³ãƒ€",
    "ãƒˆãƒ¨ã‚¿",
    "ãƒãƒ„ãƒ€",
    "ã‚¹ãƒãƒ«",
    "ãƒ€ã‚¤ãƒãƒ„",
    "ã‚¹ã‚ºã‚­",
    "ä¸‰è±è‡ªå‹•è»Š",
    "æ—¥ç”£",
]

def get_keywords() -> list[str]:
    env = os.getenv("NEWS_KEYWORDS")
    if env:
        parts = [p.strip() for p in re.split(r"[,\\n]", env) if p.strip()]
        return parts or SHEET_NAMES
    return SHEET_NAMES

def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def jst_str(fmt="%Y/%m/%d %H:%M"):
    return jst_now().strftime(fmt)

def to_hankaku_kana_ascii_digit(s: str) -> str:
    if not s:
        return ""
    s_nfkc = unicodedata.normalize("NFKC", s)
    if jaconv is not None:
        s_nfkc = jaconv.z2h(s_nfkc, kana=True, digit=True, ascii=True)
    return s_nfkc

def normalize_title_for_dup(s: str) -> str:
    if not s:
        return ""
    s = to_hankaku_kana_ascii_digit(s)
    import regex as re_u
    if re_u:
        s = re_u.sub(r'[\p{P}\p{S}\p{Z}\p{Cc}&&[^ã€ã€‘]]+', '', s)
    else:
        dash_chars = r'\\-\\u2212\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015\\uFF0D\\u30FC\\uFF70'
        pattern = (
            r'[\\s"\'\\u201C\\u201D\\u2018\\u2019\\(\\)[\\]{}<>]'
            r'|[ã€ã€‚ãƒ»,â€¦:;!?ï¼ï¼Ÿï¼/\\\\|ï¼‹+ï¼Š*.,]'
            r'|[ï¼œï¼ã€Œã€ã€ã€ã€Šã€‹ã€”ã€•ï¼»ï¼½ï½›ï½ï¼ˆï¼‰]'
            r'|[' + dash_chars + r']'
        )
        s = re.sub(pattern, "", s)
    return s

def make_driver() -> webdriver.Chrome:
    opts = Options()
    chrome_path = os.getenv("CHROME_PATH")
    if chrome_path:
        opts.binary_location = chrome_path
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)

DATE_RE = re.compile(r"(?:\\d{4}/\\d{1,2}/\\d{1,2}|\\d{1,2}/\\d{1,2})\\s*\\d{1,2}[:ï¼š]\\d{2}")

def clean_source_text(text: str) -> str:
    if not text:
        return ""
    t = text
    t = re.sub(r"[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]", "", t)
    t = DATE_RE.sub("", t)
    t = re.sub(r"^\d+\s*", "", t)
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t

def scrape_yahoo(keyword: str) -> pd.DataFrame:
    driver = make_driver()
    url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\\)', '', date_str).strip()
                try:
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = dt.strftime("%Y/%m/%d %H:%M")
                except ValueError:
                    try:
                        year = jst_now().year
                        dt = datetime.strptime(f"{year}/{ds}", "%Y/%m/%d %H:%M")
                        pub_date = dt.strftime("%Y/%m/%d %H:%M")
                    except ValueError:
                        pub_date = ds

            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div",
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                raw = el.get_text(" ", strip=True)
                txt = clean_source_text(raw)
                if txt and not txt.isdigit():
                    source = txt
                    break

            normalized_title = normalize_title_for_dup(title)

            if title and url:
                rows.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source or "Yahoo",
                    "å–å¾—æ—¥æ™‚": jst_str(), "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,
                    "ãƒã‚¸ãƒã‚¬": "", "ã‚«ãƒ†ã‚´ãƒª": "", "é‡è¤‡ç¢ºèªç”¨ã‚¿ã‚¤ãƒˆãƒ«": normalized_title,
                })
        except Exception:
            continue
    return pd.DataFrame(rows, columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "é‡è¤‡ç¢ºèªç”¨ã‚¿ã‚¤ãƒˆãƒ«"])

def download_existing_book(repo: str, tag: str, asset_name: str, token: str) -> dict[str, pd.DataFrame]:
    empty_cols = ["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "é‡è¤‡ç¢ºèªç”¨ã‚¿ã‚¤ãƒˆãƒ«"]
    dfs: dict[str, pd.DataFrame] = {sn: pd.DataFrame(columns=empty_cols) for sn in SHEET_NAMES}
    if not (repo and tag):
        print("âš ï¸ download_existing_book: repo/tag ãŒæœªè¨­å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return dfs
    base = "https://api.github.com"
    headers = {"Accept": "application/vnd.github+json"}
    if token:
        headers["Authorization"] = f"Bearer {token}"
    url_rel = f"{base}/repos/{repo}/releases/tags/{tag}"
    r = requests.get(url_rel, headers=headers)
    print(f"ğŸ” GET {url_rel} -> {r.status_code}")
    if r.status_code != 200:
        print("âš ï¸ ReleaseãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å–å¾—ã«å¤±æ•—ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs
    rel = r.json()
    asset = next((a for a in rel.get("assets", []) if a.get("name") == asset_name), None)
    if not asset:
        print(f"âš ï¸ Releaseã« {asset_name} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs
    dl_url = asset.get("browser_download_url")
    if not dl_url:
        print("âš ï¸ browser_download_url ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs
    dr = requests.get(dl_url)
    print(f"â¬‡ï¸  Download {dl_url} -> {dr.status_code}, {len(dr.content)} bytes")
    if dr.status_code != 200:
        print("âš ï¸ æ—¢å­˜Excelã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs
    with io.BytesIO(dr.content) as bio:
        try:
            book = pd.read_excel(bio, sheet_name=None, dtype=str)
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return dfs
    for sn in SHEET_NAMES:
        if sn in book:
            df = book[sn]
            for col in empty_cols:
                if col not in df.columns:
                    df[col] = ""
            dfs[sn] = df[empty_cols].copy()
    return dfs

def save_book_with_format(dfs: dict[str, pd.DataFrame], path: str):
    from openpyxl import Workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import Font, Alignment

    wb = Workbook()
    default_ws = wb.active
    if default_ws:
      wb.remove(default_ws)

    for sheet_name, df in dfs.items():
        ws = wb.create_sheet(title=sheet_name)
        headers = ["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "é‡è¤‡ç¢ºèªç”¨ã‚¿ã‚¤ãƒˆãƒ«"]
        ws.append(headers)
        
        if not df.empty:
            for row in df.itertuples(index=False):
                new_row = list(row)
                try:
                    if pd.notna(row.æŠ•ç¨¿æ—¥):
                        dt_obj = pd.to_datetime(row.æŠ•ç¨¿æ—¥, errors='coerce')
                        if not pd.isna(dt_obj):
                            new_row[2] = dt_obj
                except Exception:
                    pass
                ws.append(new_row)

        max_col = ws.max_column
        max_row = ws.max_row
        ws.auto_filter.ref = f"A1:{get_column_letter(max_col)}{max_row}"

        header_font = Font(bold=True)
        for cell in ws[1]:
            cell.font = header_font
            cell.alignment = Alignment(vertical="center")

        widths = {
            "A": 50, "B": 60, "C": 16, "D": 24, "E": 16,
            "F": 16, "G": 16, "H": 16, "I": 16,
        }
        for col, wdt in widths.items():
            if ws.max_column >= ord(col) - 64:
                ws.column_dimensions[col].width = wdt

        ws.freeze_panes = "A2"

        for row in ws.iter_rows(min_row=2, min_col=3, max_col=3):
            for cell in row:
                if isinstance(cell.value, datetime):
                    cell.number_format = 'yyyy/m/d h:mm'

    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    wb.save(path)

def classify_with_gemini(dfs: dict[str, pd.DataFrame]) -> dict[str, pd.DataFrame]:
    api_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key or genai is None:
        print("â„¹ Geminiåˆ†é¡ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAPIã‚­ãƒ¼æœªè¨­å®š or ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰ã€‚")
        return dfs

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    system_prompt = """
ã‚ãªãŸã¯æ•è…•é›‘èªŒè¨˜è€…ã§ã™ã€‚Webãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä»¥ä¸‹ã®è¦å‰‡ã§å³å¯†ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

ã€1ã€‘ãƒã‚¸ãƒã‚¬åˆ¤å®šï¼ˆå¿…ãšæ¬¡ã®ã„ãšã‚Œã‹ä¸€èªã®ã¿ï¼‰ï¼š
- ãƒã‚¸ãƒ†ã‚£ãƒ–
- ãƒã‚¬ãƒ†ã‚£ãƒ–
- ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«

ã€2ã€‘è¨˜äº‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®šï¼ˆæœ€ã‚‚é–¢é€£ãŒé«˜ã„1ã¤ã ã‘ã‚’é¸ã‚“ã§å‡ºåŠ›ã€‚ä¸¦è¨˜ç¦æ­¢ï¼‰ï¼š
- ä¼šç¤¾ï¼šä¼æ¥­ã®æ–½ç­–ã‚„ç”Ÿç”£ã€è²©å£²å°æ•°ãªã©ã€‚ãƒ‹ãƒƒã‚µãƒ³ã€ãƒˆãƒ¨ã‚¿ã€ãƒ›ãƒ³ãƒ€ã€ã‚¹ãƒãƒ«ã€ãƒãƒ„ãƒ€ã€ã‚¹ã‚ºã‚­ã€ãƒŸãƒ„ãƒ“ã‚·ã€ãƒ€ã‚¤ãƒãƒ„ã®è¨˜äº‹ã®å ´åˆã¯ () ä»˜ãã§ä¼æ¥­åã‚’è¨˜è¼‰ã€‚ãã‚Œä»¥å¤–ã¯ã€Œãã®ä»–ã€ã€‚
- è»Šï¼šã‚¯ãƒ«ãƒã®åç§°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚‚ã®ï¼ˆä¼šç¤¾åã ã‘ã®å ´åˆã¯è»Šã«åˆ†é¡ã—ãªã„ï¼‰ã€‚æ–°å‹/ç¾è¡Œ/æ—§å‹ + åç§° ã‚’ () ä»˜ãã§è¨˜è¼‰ï¼ˆä¾‹ï¼šæ–°å‹ãƒªãƒ¼ãƒ•ã€ç¾è¡Œã‚»ãƒ¬ãƒŠã€æ—§å‹ã‚¹ã‚«ã‚¤ãƒ©ã‚¤ãƒ³ï¼‰ã€‚æ—¥ç”£ä»¥å¤–ã®è»Šã®å ´åˆã¯ã€Œè»Šï¼ˆç«¶åˆï¼‰ã€ã¨è¨˜è¼‰ã€‚
- æŠ€è¡“ï¼ˆEVï¼‰ï¼šé›»æ°—è‡ªå‹•è»Šã®æŠ€è¡“ã«é–¢ã‚ã‚‹ã‚‚ã®ï¼ˆãŸã ã—ãƒãƒƒãƒ†ãƒªãƒ¼å·¥å ´å»ºè¨­ã‚„ä¼æ¥­ã®æ–½ç­–ã¯å«ã¾ãªã„ï¼‰ã€‚
- æŠ€è¡“ï¼ˆe-POWERï¼‰ï¼še-POWERã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- æŠ€è¡“ï¼ˆe-4ORCEï¼‰ï¼š4WDã‚„2WDã€AWDã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- æŠ€è¡“ï¼ˆAD/ADASï¼‰ï¼šè‡ªå‹•é‹è»¢ã‚„å…ˆé€²é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- æŠ€è¡“ï¼šä¸Šè¨˜ä»¥å¤–ã®æŠ€è¡“ã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚¹ãƒãƒ¼ãƒ„ï¼šF1ã‚„ãƒ©ãƒªãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ©Eãªã©ã€è‡ªå‹•è»Šãƒ¬ãƒ¼ã‚¹ã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- æ ªå¼ï¼šæ ªå¼ç™ºè¡Œã‚„æ ªä¾¡ã®å€¤å‹•ãã€æŠ•è³‡ã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- æ”¿æ²»ãƒ»çµŒæ¸ˆï¼šæ”¿æ²»å®¶ã‚„é¸æŒ™ã€ç¨é‡‘ã€çµŒæ¸ˆã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- ã‚¹ãƒãƒ¼ãƒ„ï¼šé‡çƒã‚„ã‚µãƒƒã‚«ãƒ¼ã€ãƒãƒ¬ãƒ¼ãƒœãƒ¼ãƒ«ãªã©è‡ªå‹•è»Šä»¥å¤–ã®ã‚¹ãƒãƒ¼ãƒ„ã«é–¢ã‚ã‚‹ã‚‚ã®ã€‚
- ãã®ä»–ï¼šä¸Šè¨˜ã«å«ã¾ã‚Œãªã„ã‚‚ã®ã€‚

ã€å‡ºåŠ›è¦ä»¶ã€‘
- **JSONé…åˆ—**ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆä½™è¨ˆãªæ–‡ç« ã‚„æ³¨é‡ˆã¯å‡ºåŠ›ã—ãªã„ï¼‰ã€‚
- å„è¦ç´ ã¯æ¬¡ã®å½¢å¼ï¼š{"row": è¡Œç•ªå·, "sentiment": "ãƒã‚¸ãƒ†ã‚£ãƒ–|ãƒã‚¬ãƒ†ã‚£ãƒ–|ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "category": "ã‚«ãƒ†ã‚´ãƒªå"}
- å…¥åŠ›ã®ã€Œã‚¿ã‚¤ãƒˆãƒ«ã€æ–‡å­—åˆ—ã¯ä¸€åˆ‡å¤‰æ›´ã—ãªã„ã“ã¨ï¼ˆå‡ºåŠ›ã«ã¯å«ã‚ãªãã¦è‰¯ã„ï¼‰ã€‚
""".strip()

    classified_dfs = {}
    for sheet_name, df in dfs.items():
        df_to_classify = df[(df["ãƒã‚¸ãƒã‚¬"] == "") | (df["ã‚«ãƒ†ã‚´ãƒª"] == "")]

        if df_to_classify.empty:
            print(f"â„¹ {sheet_name}: åˆ†é¡å¯¾è±¡ã®è¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            classified_dfs[sheet_name] = df
            continue

        print(f"âœ¨ {sheet_name}: {len(df_to_classify)}ä»¶ã‚’Geminiã§åˆ†é¡ã—ã¾ã™ã€‚")
        df_to_classify = df_to_classify.reset_index(drop=True)

        batch_size = 40
        for start in range(0, len(df_to_classify), batch_size):
            batch = df_to_classify.iloc[start:start + batch_size]
            payload = [{"row": i, "title": t} for i, t in batch.loc[:, ["ã‚¿ã‚¤ãƒˆãƒ«"]].itertuples(index=True)]

            try:
                prompt = system_prompt + "\n\n" + json.dumps(payload, ensure_ascii=False, indent=2)
                resp = model.generate_content(prompt)
                text = (resp.text or "").strip()

                import regex as re_u
                m = re_u.search(r'\[.*\]', text, flags=re_u.DOTALL)
                json_text = m.group(0) if m else text
                result = json.loads(json_text)

                for obj in result:
                    try:
                        idx = int(obj.get("row"))
                        sentiment = str(obj.get("sentiment", "")).strip()
                        category = str(obj.get("category", "")).strip()
                        if sentiment and category:
                            df.loc[df_to_classify.index[idx], "ãƒã‚¸ãƒã‚¬"] = sentiment
                            df.loc[df_to_classify.index[idx], "ã‚«ãƒ†ã‚´ãƒª"] = category
                    except Exception as e:
                        print(f"âš  Geminiå¿œç­”ã®è§£æã«å¤±æ•—: {e}")
            except Exception as e:
                print(f"âš  Gemini APIå‘¼ã³å‡ºã—ã«å¤±æ•—: {e}")

        classified_dfs[sheet_name] = df
    return classified_dfs

def main():
    keywords = get_keywords()
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§: {', '.join(keywords)}")

    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")
    dfs_old = download_existing_book(repo, RELEASE_TAG, ASSET_NAME, token)

    dfs_merged: dict[str, pd.DataFrame] = {}
    for kw in keywords:
        df_old = dfs_old.get(kw, pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒã‚¸ãƒã‚¬", "ã‚«ãƒ†ã‚´ãƒª", "é‡è¤‡ç¢ºèªç”¨ã‚¿ã‚¤ãƒˆãƒ«"]))
        df_new = scrape_yahoo(kw)

        df_old['æŠ•ç¨¿æ—¥'] = df_old['æŠ•ç¨¿æ—¥'].astype(str)
        df_new['æŠ•ç¨¿æ—¥'] = df_new['æŠ•ç¨¿æ—¥'].astype(str)
        
        df_all = pd.concat([df_old, df_new], ignore_index=True)
        if not df_all.empty:
            df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="first")
        dfs_merged[kw] = df_all
        print(f"  - {kw}: æ—¢å­˜ {len(df_old)} ä»¶ + æ–°è¦ {len(df_new)} ä»¶ â†’ åˆè¨ˆ {len(df_all)} ä»¶")

    dfs_classified = classify_with_gemini(dfs_merged)

    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", ASSET_NAME)
    save_book_with_format(dfs_classified, out_path)

    print(f"âœ… Excelå‡ºåŠ›: {out_path}")
    if repo:
        owner_repo = repo
    else:
        owner_repo = "<OWNER>/<REPO>"
    print(f"ğŸ”— å›ºå®šDL: https://github.com/{owner_repo}/releases/download/{RELEASE_TAG}/{ASSET_NAME}")

if __name__ == "__main__":
    main()
