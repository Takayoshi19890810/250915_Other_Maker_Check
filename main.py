# -*- coding: utf-8 -*-
import os
import re
import io
import time
import argparse
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ===== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def jst_str(fmt="%Y/%m/%d %H:%M"):
    return jst_now().strftime(fmt)

DEFAULT_KEYWORD = "ãƒ›ãƒ³ãƒ€"   # NEWS_KEYWORD ç’°å¢ƒå¤‰æ•° or --keyword ã§ä¸Šæ›¸ã
RELEASE_TAG = "news-latest"
ASSET_NAME = "yahoo_news.xlsx"
SHEET_NAME = "news"


# ===== Chromeï¼ˆheadlessï¼‰ =====
def make_driver() -> webdriver.Chrome:
    opts = Options()
    chrome_path = os.getenv("CHROME_PATH")  # Actionsã§æ³¨å…¥
    if chrome_path:
        opts.binary_location = chrome_path
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)


# ===== å¼•ç”¨å…ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— =====
DATE_RE = re.compile(r"(?:\d{4}/\d{1,2}/\d{1,2}|\d{1,2}/\d{1,2})\s*\d{1,2}[:ï¼š]\d{2}")

def clean_source_text(text: str) -> str:
    if not text:
        return ""
    t = text
    t = re.sub(r"[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]", "", t)      # ï¼ˆï¼‰å†…å‰Šé™¤
    t = DATE_RE.sub("", t)                       # æ—¥ä»˜+æ™‚åˆ»å‰Šé™¤
    t = re.sub(r"^\d+\s*", "", t)                # å…ˆé ­ã®ç•ªå·ï¼‹ç©ºç™½
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t


# ===== Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ =====
def get_yahoo_news(keyword: str) -> pd.DataFrame:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ¤œç´¢ï¼‰ã‹ã‚‰ ã‚¿ã‚¤ãƒˆãƒ«/URL/æŠ•ç¨¿æ—¥/å¼•ç”¨å…ƒ ã‚’å–å¾—ï¼ˆ1ãƒšãƒ¼ã‚¸ï¼‰
    """
    driver = make_driver()
    url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(url)
    time.sleep(5)  # åˆæœŸæç”»å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            # æŠ•ç¨¿æ—¥ï¼šYYYY/MM/DD HH:MM ã«æƒãˆã‚‰ã‚Œã‚‹å ´åˆã¯æƒãˆã€ãã‚Œä»¥å¤–ã¯åŸæ–‡
            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = dt.strftime("%Y/%m/%d %H:%M")
                except Exception:
                    pub_date = ds

            # å¼•ç”¨å…ƒï¼ˆåª’ä½“ï¼‹ã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³
            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div"
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                raw = el.get_text(" ", strip=True)
                txt = clean_source_text(raw)
                if txt and not txt.isdigit():
                    source = txt
                    break

            if title and url:
                rows.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": pub_date,
                    "å¼•ç”¨å…ƒ": source or "Yahoo",
                    "å–å¾—æ—¥æ™‚": jst_str(),            # ã„ã¤å–å¾—ã—ãŸã‹ï¼ˆè¿½è¨˜é‹ç”¨ã®ãŸã‚ï¼‰
                    "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,        # å°†æ¥ãƒãƒ«ãƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ™‚ã«å½¹ç«‹ã¤
                })
        except Exception:
            continue

    return pd.DataFrame(rows, columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])


# ===== Releaseã‹ã‚‰æ—¢å­˜Excelã‚’å–å¾— =====
def download_existing_from_release(repo: str, tag: str, asset_name: str, token: str) -> pd.DataFrame:
    """Release(tag)ã«å­˜åœ¨ã™ã‚Œã°Excelã‚’DLã—ã¦DFã§è¿”ã™ã€‚ç„¡ã‘ã‚Œã°ç©ºDFã€‚"""
    if not (repo and tag and token):
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])

    base = "https://api.github.com"
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

    r = requests.get(f"{base}/repos/{repo}/releases/tags/{tag}", headers=headers)
    if r.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])
    rel = r.json()

    asset = next((a for a in rel.get("assets", []) if a.get("name") == asset_name), None)
    if not asset:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])

    headers_dl = headers | {"Accept": "application/octet-stream"}
    dr = requests.get(asset["url"], headers_dl)
    if dr.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])

    with io.BytesIO(dr.content) as bio:
        try:
            df = pd.read_excel(bio, sheet_name=SHEET_NAME)
            return df[["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]].copy()
        except Exception:
            return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])


# ===== ä¿å­˜ï¼ˆã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼/åˆ—å¹…/ãƒ•ãƒªãƒ¼ã‚ºå¯¾å¿œï¼‰ =====
def save_with_format(df: pd.DataFrame, path: str):
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import Font, Alignment
    with pd.ExcelWriter(path, engine="openpyxl") as w:
        df.to_excel(w, index=False, sheet_name=SHEET_NAME)
        ws = w.book[SHEET_NAME]

        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã«ä¸¦ã¹æ›¿ãˆãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒœã‚¿ãƒ³ï¼‰
        max_col = ws.max_column
        max_row = ws.max_row
        ws.auto_filter.ref = f"A1:{get_column_letter(max_col)}{max_row}"

        # ãƒ˜ãƒƒãƒ€ãƒ¼å¤ªå­—ï¼†ä¸­å¤®å¯„ã›
        header_font = Font(bold=True)
        for cell in ws[1]:
            cell.font = header_font
            cell.alignment = Alignment(vertical="center")

        # åˆ—å¹…ã®è»½èª¿æ•´
        widths = {
            "A": 50,  # ã‚¿ã‚¤ãƒˆãƒ«
            "B": 60,  # URL
            "C": 16,  # æŠ•ç¨¿æ—¥
            "D": 24,  # å¼•ç”¨å…ƒ
            "E": 16,  # å–å¾—æ—¥æ™‚
            "F": 16,  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        }
        for col, wdt in widths.items():
            if ws.max_column >= ord(col) - 64:
                ws.column_dimensions[col].width = wdt

        # ãƒ˜ãƒƒãƒ€ãƒ¼å›ºå®šï¼ˆ1è¡Œç›®ï¼‰
        ws.freeze_panes = "A2"


# ===== ãƒ¡ã‚¤ãƒ³ =====
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--keyword", type=str, default=None, help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœªæŒ‡å®šãªã‚‰ç’°å¢ƒå¤‰æ•°NEWS_KEYWORDã€ãªã‘ã‚Œã°ãƒ›ãƒ³ãƒ€ï¼‰")
    args = ap.parse_args()

    keyword = args.keyword or os.getenv("NEWS_KEYWORD") or DEFAULT_KEYWORD
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")

    # 1) æœ€æ–°å–å¾—
    df_new = get_yahoo_news(keyword)

    # 2) æ—¢å­˜ï¼ˆå›ºå®šReleaseè³‡ç”£ï¼‰ã¨ãƒãƒ¼ã‚¸ï¼ˆæ—¢å­˜å„ªå…ˆï¼æ–°è¦ã¯æœ«å°¾ã«ä»˜ãï¼‰
    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")
    df_old = download_existing_from_release(repo, RELEASE_TAG, ASSET_NAME, token)

    df_all = pd.concat([df_old, df_new], ignore_index=True)
    if not df_all.empty:
        df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="first")
        # ä¸¦ã¹æ›¿ãˆã¯ã—ãªã„ï¼šæ—¢å­˜ã®é †åºã‚’ä¿æŒã—ã€æ–°è¦ã¯æœ«å°¾ã«è¿½è¨˜ã•ã‚Œã‚‹

    # 3) ä¿å­˜ï¼ˆå˜ä¸€ã‚·ãƒ¼ãƒˆ news, ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä»˜ãï¼‰
    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", ASSET_NAME)
    save_with_format(df_all, out_path)

    print(f"âœ… Excelå‡ºåŠ›: {out_path}ï¼ˆåˆè¨ˆ {len(df_all)} ä»¶ã€ã†ã¡æ–°è¦ {len(df_new)} ä»¶ï¼‰")
    print(f"ğŸ”— å›ºå®šDL: https://github.com/<OWNER>/<REPO>/releases/download/{RELEASE_TAG}/{ASSET_NAME}")


if __name__ == "__main__":
    main()
