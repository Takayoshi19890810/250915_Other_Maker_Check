# -*- coding: utf-8 -*-
import os
import re
import io
import time
import json
import unicodedata
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import openpyxl
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment

try:
    import google.generativeai as genai
except Exception:
    genai = None

try:
    import jaconv # pip install jaconv
except Exception:
    jaconv = None

# ===== Ë®≠ÂÆö =====
RELEASE_TAG = "news-latest"
ASSET_NAME = "yahoo_news.xlsx"
SHEET_NAMES = [
    "„Éõ„É≥„ÉÄ",
    "„Éà„É®„Çø",
    "„Éû„ÉÑ„ÉÄ",
    "„Çπ„Éê„É´",
    "„ÉÄ„Ç§„Éè„ÉÑ",
    "„Çπ„Ç∫„Ç≠",
    "‰∏âËè±Ëá™ÂãïËªä",
    "Êó•Áî£",
]

def get_keywords() -> list[str]:
    env = os.getenv("NEWS_KEYWORDS")
    if env:
        parts = [p.strip() for p in re.split(r"[,\\n]", env) if p.strip()]
        return parts or SHEET_NAMES
    return SHEET_NAMES

def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def jst_str(fmt="%Y/%m/%d %H:%M"):
    return jst_now().strftime(fmt)

def to_hankaku_kana_ascii_digit(s: str) -> str:
    if not s:
        return ""
    s_nfkc = unicodedata.normalize("NFKC", s)
    if jaconv is not None:
        s_nfkc = jaconv.z2h(s_nfkc, kana=True, digit=True, ascii=True)
    return s_nfkc

def normalize_title_for_dup(s: str) -> str:
    if not s:
        return ""
    s = to_hankaku_kana_ascii_digit(s)
    import regex as re_u
    if re_u:
        s = re_u.sub(r'[\p{P}\p{S}\p{Z}\p{Cc}&&[^„Äê„Äë]]+', '', s)
    else:
        dash_chars = r'\\-\\u2212\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015\\uFF0D\\u30FC\\uFF70'
        pattern = (
            r'[\\s"\'\\u201C\\u201D\\u2018\\u2019\\(\\)[\\]{}<>]'
            r'|[„ÄÅ„ÄÇ„Éª,‚Ä¶:;!?ÔºÅÔºüÔºè/\\\\|Ôºã+Ôºä*.,]'
            r'|[ÔºúÔºû„Äå„Äç„Äé„Äè„Ää„Äã„Äî„ÄïÔºªÔºΩÔΩõÔΩùÔºàÔºâ]'
            r'|[' + dash_chars + r']'
        )
        s = re.sub(pattern, "", s)
    return s

def make_driver() -> webdriver.Chrome:
    opts = Options()
    chrome_path = os.getenv("CHROME_PATH")
    if chrome_path:
        opts.binary_location = chrome_path
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)

DATE_RE = re.compile(r"(?:\\d{4}/\\d{1,2}/\\d{1,2}|\\d{1,2}/\\d{1,2})\\s*\\d{1,2}[:Ôºö]\\d{2}")

def clean_source_text(text: str) -> str:
    if not text:
        return ""
    t = text
    t = re.sub(r"[Ôºà(][^Ôºâ)]+[Ôºâ)]", "", t)
    t = DATE_RE.sub("", t)
    t = re.sub(r"^\d+\s*", "", t)
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t

def scrape_yahoo(keyword: str) -> pd.DataFrame:
    driver = make_driver()
    url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            pub_date = "ÂèñÂæó‰∏çÂèØ"
            if date_str:
                ds = re.sub(r'\\([ÊúàÁÅ´Ê∞¥Êú®ÈáëÂúüÊó•]\\)', '', date_str).strip()
                try:
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = dt.strftime("%Y/%m/%d %H:%M")
                except ValueError:
                    try:
                        year = jst_now().year
                        dt = datetime.strptime(f"{year}/{ds}", "%Y/%m/%d %H:%M")
                        pub_date = dt.strftime("%Y/%m/%d %H:%M")
                    except ValueError:
                        pub_date = ds

            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div",
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                raw = el.get_text(" ", strip=True)
                txt = clean_source_text(raw)
                if txt and not txt.isdigit():
                    source = txt
                    break

            normalized_title = normalize_title_for_dup(title)

            if title and url:
                rows.append({
                    "„Çø„Ç§„Éà„É´": title, "URL": url, "ÊäïÁ®øÊó•": pub_date, "ÂºïÁî®ÂÖÉ": source or "Yahoo",
                    "ÂèñÂæóÊó•ÊôÇ": jst_str(), "Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ": keyword,
                    "„Éù„Ç∏„Éç„Ç¨": "", "„Ç´„ÉÜ„Ç¥„É™": "", "ÈáçË§áÁ¢∫Ë™çÁî®„Çø„Ç§„Éà„É´": normalized_title,
                })
        except Exception:
            continue
    return pd.DataFrame(rows, columns=["„Çø„Ç§„Éà„É´", "URL", "ÊäïÁ®øÊó•", "ÂºïÁî®ÂÖÉ", "ÂèñÂæóÊó•ÊôÇ", "Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ", "„Éù„Ç∏„Éç„Ç¨", "„Ç´„ÉÜ„Ç¥„É™", "ÈáçË§áÁ¢∫Ë™çÁî®„Çø„Ç§„Éà„É´"])

def download_existing_book(repo: str, tag: str, asset_name: str, token: str) -> dict[str, pd.DataFrame]:
    empty_cols = ["„Çø„Ç§„Éà„É´", "URL", "ÊäïÁ®øÊó•", "ÂºïÁî®ÂÖÉ", "ÂèñÂæóÊó•ÊôÇ", "Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ", "„Éù„Ç∏„Éç„Ç¨", "„Ç´„ÉÜ„Ç¥„É™", "ÈáçË§áÁ¢∫Ë™çÁî®„Çø„Ç§„Éà„É´"]
    dfs: dict[str, pd.DataFrame] = {sn: pd.DataFrame(columns=empty_cols) for sn in SHEET_NAMES}
    if not (repo and tag):
        print("‚ö†Ô∏è download_existing_book: repo/tag „ÅåÊú™Ë®≠ÂÆö„ÅÆ„Åü„ÇÅ„Çπ„Ç≠„ÉÉ„Éó")
        return dfs
    base = "https://api.github.com"
    headers = {"Accept": "application/vnd.github+json"}
    if token:
        headers["Authorization"] = f"Bearer {token}"
    url_rel = f"{base}/repos/{repo}/releases/tags/{tag}"
    r = requests.get(url_rel, headers=headers)
    print(f"üîé GET {url_rel} -> {r.status_code}")
    if r.status_code != 200:
        print("‚ö†Ô∏è Release„ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ„Åã„ÄÅÂèñÂæó„Å´Â§±Êïó„ÄÇÊó¢Â≠ò„ÅØÁ©∫„Å®„Åó„Å¶Á∂öË°å„Åó„Åæ„Åô„ÄÇ")
        return dfs
    rel = r.json()
    asset = next((a for a in rel.get("assets", []) if a.get("name") == asset_name), None)
    if not asset:
        print(f"‚ö†Ô∏è Release„Å´ {asset_name} „ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì„ÄÇÊó¢Â≠ò„ÅØÁ©∫„Å®„Åó„Å¶Á∂öË°å„Åó„Åæ„Åô„ÄÇ")
        return dfs
    dl_url = asset.get("browser_download_url")
    if not dl_url:
        print("‚ö†Ô∏è browser_download_url „ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇÊó¢Â≠ò„ÅØÁ©∫„Å®„Åó„Å¶Á∂öË°å„Åó„Åæ„Åô„ÄÇ")
        return dfs
    dr = requests.get(dl_url)
    print(f"‚¨áÔ∏è  Download {dl_url} -> {dr.status_code}, {len(dr.content)} bytes")
    if dr.status_code != 200:
        print("‚ö†Ô∏è Êó¢Â≠òExcel„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Å´Â§±Êïó„ÄÇÊó¢Â≠ò„ÅØÁ©∫„Å®„Åó„Å¶Á∂öË°å„Åó„Åæ„Åô„ÄÇ")
        return dfs
    with io.BytesIO(dr.content) as bio:
        try:
            book = pd.read_excel(bio, sheet_name=None, dtype=str)
        except Exception as e:
            print(f"‚ö†Ô∏è Êó¢Â≠òExcel„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó: {e}")
            return dfs
    for sn in SHEET_NAMES:
        if sn in book:
            df = book[sn]
            for col in empty_cols:
                if col not in df.columns:
                    df[col] = ""
            dfs[sn] = df[empty_cols].copy()
    return dfs

def save_book_with_format(dfs: dict[str, pd.DataFrame], path: str):
    from openpyxl import Workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import Font, Alignment

    wb = Workbook()
    default_ws = wb.active
    if default_ws:
      wb.remove(default_ws)

    for sheet_name, df in dfs.items():
        ws = wb.create_sheet(title=sheet_name)
        headers = ["„Çø„Ç§„Éà„É´", "URL", "ÊäïÁ®øÊó•", "ÂºïÁî®ÂÖÉ", "ÂèñÂæóÊó•ÊôÇ", "Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ", "„Éù„Ç∏„Éç„Ç¨", "„Ç´„ÉÜ„Ç¥„É™", "ÈáçË§áÁ¢∫Ë™çÁî®„Çø„Ç§„Éà„É´"]
        ws.append(headers)
        
        if not df.empty:
            for row in df.itertuples(index=False):
                new_row = list(row)
                try:
                    if pd.notna(row.ÊäïÁ®øÊó•):
                        dt_obj = pd.to_datetime(row.ÊäïÁ®øÊó•, errors='coerce')
                        if not pd.isna(dt_obj):
                            new_row[2] = dt_obj
                except Exception:
                    pass
                ws.append(new_row)

        max_col = ws.max_column
        max_row = ws.max_row
        ws.auto_filter.ref = f"A1:{get_column_letter(max_col)}{max_row}"

        header_font = Font(bold=True)
        for cell in ws[1]:
            cell.font = header_font
            cell.alignment = Alignment(vertical="center")

        widths = {
            "A": 50, "B": 60, "C": 16, "D": 24, "E": 16,
            "F": 16, "G": 16, "H": 16, "I": 16,
        }
        for col, wdt in widths.items():
            if ws.max_column >= ord(col) - 64:
                ws.column_dimensions[col].width = wdt

        ws.freeze_panes = "A2"

        for row in ws.iter_rows(min_row=2, min_col=3, max_col=3):
            for cell in row:
                if isinstance(cell.value, datetime):
                    cell.number_format = 'yyyy/m/d h:mm'

    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    wb.save(path)

def classify_with_gemini(dfs: dict[str, pd.DataFrame]) -> dict[str, pd.DataFrame]:
    api_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not api_key or genai is None:
        print("‚Ñπ GeminiÂàÜÈ°û„ÅØ„Çπ„Ç≠„ÉÉ„ÉóÔºàAPI„Ç≠„ÉºÊú™Ë®≠ÂÆö or „É©„Ç§„Éñ„É©„É™Êú™„Ç§„É≥„Çπ„Éà„Éº„É´Ôºâ„ÄÇ")
        return dfs

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    system_prompt = """
„ÅÇ„Å™„Åü„ÅØÊïèËÖïÈõëË™åË®òËÄÖ„Åß„Åô„ÄÇWeb„Éã„É•„Éº„Çπ„ÅÆ„Çø„Ç§„Éà„É´„Çí‰ª•‰∏ã„ÅÆË¶èÂâá„ÅßÂé≥ÂØÜ„Å´ÂàÜÈ°û„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê1„Äë„Éù„Ç∏„Éç„Ç¨Âà§ÂÆöÔºàÂøÖ„ÅöÊ¨°„ÅÆ„ÅÑ„Åö„Çå„Åã‰∏ÄË™û„ÅÆ„ÅøÔºâÔºö
- „Éù„Ç∏„ÉÜ„Ç£„Éñ
- „Éç„Ç¨„ÉÜ„Ç£„Éñ
- „Éã„É•„Éº„Éà„É©„É´

„Äê2„ÄëË®ò‰∫ã„ÅÆ„Ç´„ÉÜ„Ç¥„É™„ÉºÂà§ÂÆöÔºàÊúÄ„ÇÇÈñ¢ÈÄ£„ÅåÈ´ò„ÅÑ1„Å§„Å†„Åë„ÇíÈÅ∏„Çì„ÅßÂá∫Âäõ„ÄÇ‰∏¶Ë®òÁ¶ÅÊ≠¢ÔºâÔºö
- ‰ºöÁ§æÔºö‰ºÅÊ•≠„ÅÆÊñΩÁ≠ñ„ÇÑÁîüÁî£„ÄÅË≤©Â£≤Âè∞Êï∞„Å™„Å©„ÄÇ„Éã„ÉÉ„Çµ„É≥„ÄÅ„Éà„É®„Çø„ÄÅ„Éõ„É≥„ÉÄ„ÄÅ„Çπ„Éê„É´„ÄÅ„Éû„ÉÑ„ÉÄ„ÄÅ„Çπ„Ç∫„Ç≠„ÄÅ„Éü„ÉÑ„Éì„Ç∑„ÄÅ„ÉÄ„Ç§„Éè„ÉÑ„ÅÆË®ò‰∫ã„ÅÆÂ†¥Âêà„ÅØ () ‰ªò„Åç„Åß‰ºÅÊ•≠Âêç„ÇíË®òËºâ„ÄÇ„Åù„Çå‰ª•Â§ñ„ÅØ„Äå„Åù„ÅÆ‰ªñ„Äç„ÄÇ
- ËªäÔºö„ÇØ„É´„Éû„ÅÆÂêçÁß∞„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„ÇÇ„ÅÆÔºà‰ºöÁ§æÂêç„Å†„Åë„ÅÆÂ†¥Âêà„ÅØËªä„Å´ÂàÜÈ°û„Åó„Å™„ÅÑÔºâ„ÄÇÊñ∞Âûã/ÁèæË°å/ÊóßÂûã + ÂêçÁß∞ „Çí () ‰ªò„Åç„ÅßË®òËºâÔºà‰æãÔºöÊñ∞Âûã„É™„Éº„Éï„ÄÅÁèæË°å„Çª„É¨„Éä„ÄÅÊóßÂûã„Çπ„Ç´„Ç§„É©„Ç§„É≥Ôºâ„ÄÇÊó•Áî£‰ª•Â§ñ„ÅÆËªä„ÅÆÂ†¥Âêà„ÅØ„ÄåËªäÔºàÁ´∂ÂêàÔºâ„Äç„Å®Ë®òËºâ„ÄÇ
- ÊäÄË°ìÔºàEVÔºâÔºöÈõªÊ∞óËá™ÂãïËªä„ÅÆÊäÄË°ì„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆÔºà„Åü„Å†„Åó„Éê„ÉÉ„ÉÜ„É™„ÉºÂ∑•Â†¥Âª∫Ë®≠„ÇÑ‰ºÅÊ•≠„ÅÆÊñΩÁ≠ñ„ÅØÂê´„Åæ„Å™„ÅÑÔºâ„ÄÇ
- ÊäÄË°ìÔºàe-POWERÔºâÔºöe-POWER„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- ÊäÄË°ìÔºàe-4ORCEÔºâÔºö4WD„ÇÑ2WD„ÄÅAWD„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- ÊäÄË°ìÔºàAD/ADASÔºâÔºöËá™ÂãïÈÅãËª¢„ÇÑÂÖàÈÄ≤ÈÅãËª¢„Ç∑„Çπ„ÉÜ„É†„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- ÊäÄË°ìÔºö‰∏äË®ò‰ª•Â§ñ„ÅÆÊäÄË°ì„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- „É¢„Éº„Çø„Éº„Çπ„Éù„Éº„ÉÑÔºöF1„ÇÑ„É©„É™„Éº„ÄÅ„Éï„Ç©„Éº„Éü„É•„É©E„Å™„Å©„ÄÅËá™ÂãïËªä„É¨„Éº„Çπ„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- Ê†™ÂºèÔºöÊ†™ÂºèÁô∫Ë°å„ÇÑÊ†™‰æ°„ÅÆÂÄ§Âãï„Åç„ÄÅÊäïË≥á„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- ÊîøÊ≤ª„ÉªÁµåÊ∏àÔºöÊîøÊ≤ªÂÆ∂„ÇÑÈÅ∏Êåô„ÄÅÁ®éÈáë„ÄÅÁµåÊ∏à„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- „Çπ„Éù„Éº„ÉÑÔºöÈáéÁêÉ„ÇÑ„Çµ„ÉÉ„Ç´„Éº„ÄÅ„Éê„É¨„Éº„Éú„Éº„É´„Å™„Å©Ëá™ÂãïËªä‰ª•Â§ñ„ÅÆ„Çπ„Éù„Éº„ÉÑ„Å´Èñ¢„Çè„Çã„ÇÇ„ÅÆ„ÄÇ
- „Åù„ÅÆ‰ªñÔºö‰∏äË®ò„Å´Âê´„Åæ„Çå„Å™„ÅÑ„ÇÇ„ÅÆ„ÄÇ

„ÄêÂá∫ÂäõË¶Å‰ª∂„Äë
- **JSONÈÖçÂàó**„ÅÆ„Åø„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà‰ΩôË®à„Å™ÊñáÁ´†„ÇÑÊ≥®Èáà„ÅØÂá∫Âäõ„Åó„Å™„ÅÑÔºâ„ÄÇ
- ÂêÑË¶ÅÁ¥†„ÅØÊ¨°„ÅÆÂΩ¢ÂºèÔºö{"row": Ë°åÁï™Âè∑, "sentiment": "„Éù„Ç∏„ÉÜ„Ç£„Éñ|„Éç„Ç¨„ÉÜ„Ç£„Éñ|„Éã„É•„Éº„Éà„É©„É´", "category": "„Ç´„ÉÜ„Ç¥„É™Âêç"}
- ÂÖ•Âäõ„ÅÆ„Äå„Çø„Ç§„Éà„É´„ÄçÊñáÂ≠óÂàó„ÅØ‰∏ÄÂàáÂ§âÊõ¥„Åó„Å™„ÅÑ„Åì„Å®ÔºàÂá∫Âäõ„Å´„ÅØÂê´„ÇÅ„Å™„Åè„Å¶ËâØ„ÅÑÔºâ„ÄÇ
""".strip()

    classified_dfs = {}
    for sheet_name, df in dfs.items():
        df_to_classify = df[(df["„Éù„Ç∏„Éç„Ç¨"] == "") | (df["„Ç´„ÉÜ„Ç¥„É™"] == "")]

        if df_to_classify.empty:
            print(f"‚Ñπ {sheet_name}: ÂàÜÈ°ûÂØæË±°„ÅÆË°å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
            classified_dfs[sheet_name] = df
            continue

        print(f"‚ú® {sheet_name}: {len(df_to_classify)}‰ª∂„ÇíGemini„ÅßÂàÜÈ°û„Åó„Åæ„Åô„ÄÇ")
        df_to_classify = df_to_classify.reset_index(drop=True)

        batch_size = 40
        for start in range(0, len(df_to_classify), batch_size):
            batch = df_to_classify.iloc[start:start + batch_size]
            payload = [{"row": i, "title": t} for i, t in batch.loc[:, ["„Çø„Ç§„Éà„É´"]].itertuples(index=True)]

            try:
                prompt = system_prompt + "\n\n" + json.dumps(payload, ensure_ascii=False, indent=2)
                resp = model.generate_content(prompt)
                text = (resp.text or "").strip()

                import regex as re_u
                m = re_u.search(r'\[.*\]', text, flags=re_u.DOTALL)
                json_text = m.group(0) if m else text
                result = json.loads(json_text)

                for obj in result:
                    try:
                        idx = int(obj.get("row"))
                        sentiment = str(obj.get("sentiment", "")).strip()
                        category = str(obj.get("category", "")).strip()
                        if sentiment and category:
                            df.loc[df_to_classify.index[idx], "„Éù„Ç∏„Éç„Ç¨"] = sentiment
                            df.loc[df_to_classify.index[idx], "„Ç´„ÉÜ„Ç¥„É™"] = category
                    except Exception as e:
                        print(f"‚ö† GeminiÂøúÁ≠î„ÅÆËß£Êûê„Å´Â§±Êïó: {e}")
            except Exception as e:
                print(f"‚ö† Gemini APIÂëº„Å≥Âá∫„Åó„Å´Â§±Êïó: {e}")

        classified_dfs[sheet_name] = df
    return classified_dfs

def main():
    keywords = get_keywords()
    print(f"üîé „Ç≠„Éº„ÉØ„Éº„Éâ‰∏ÄË¶ß: {', '.join(keywords)}")

    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")
    dfs_old = download_existing_book(repo, RELEASE_TAG, ASSET_NAME, token)

    dfs_merged: dict[str, pd.DataFrame] = {}
    for kw in keywords:
        df_old = dfs_old.get(kw, pd.DataFrame(columns=["„Çø„Ç§„Éà„É´", "URL", "ÊäïÁ®øÊó•", "ÂºïÁî®ÂÖÉ", "ÂèñÂæóÊó•ÊôÇ", "Ê§úÁ¥¢„Ç≠„Éº„ÉØ„Éº„Éâ", "„Éù„Ç∏„Éç„Ç¨", "„Ç´„ÉÜ„Ç¥„É™", "ÈáçË§áÁ¢∫Ë™çÁî®„Çø„Ç§„Éà„É´"]))
        df_new = scrape_yahoo(kw)

        df_old['ÊäïÁ®øÊó•'] = df_old['ÊäïÁ®øÊó•'].astype(str)
        df_new['ÊäïÁ®øÊó•'] = df_new['ÊäïÁ®øÊó•'].astype(str)
        
        df_all = pd.concat([df_old, df_new], ignore_index=True)
        if not df_all.empty:
            df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="first")
        dfs_merged[kw] = df_all
        print(f"  - {kw}: Êó¢Â≠ò {len(df_old)} ‰ª∂ + Êñ∞Ë¶è {len(df_new)} ‰ª∂ ‚Üí ÂêàË®à {len(df_all)} ‰ª∂")

    dfs_classified = classify_with_gemini(dfs_merged)

    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", ASSET_NAME)
    save_book_with_format(dfs_classified, out_path)

    print(f"‚úÖ ExcelÂá∫Âäõ: {out_path}")
    if repo:
        owner_repo = repo
    else:
        owner_repo = "<OWNER>/<REPO>"
    print(f"üîó Âõ∫ÂÆöDL: https://github.com/{owner_repo}/releases/download/{RELEASE_TAG}/{ASSET_NAME}")

if __name__ == "__main__":
    main()
