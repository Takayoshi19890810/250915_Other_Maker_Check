# -*- coding: utf-8 -*-
import os
import re
import io
import json
import time
import argparse
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# =========================
# è¨­å®š
# =========================
def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def ym_tag():
    return jst_now().strftime("%Y-%m")

def monthly_excel_name():
    return f"yahoo_news_{jst_now().strftime('%Y-%m')}.xlsx"

DEFAULT_KEYWORD = "ãƒ›ãƒ³ãƒ€"   # ç’°å¢ƒå¤‰æ•° NEWS_KEYWORD / å¼•æ•° --keyword ã§ä¸Šæ›¸ãå¯


# =========================
# ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹Chromeç”¨Driver
# =========================
def make_driver() -> webdriver.Chrome:
    options = Options()
    # GitHub Actionsç”¨ã«headless(new) + Chromeãƒ‘ã‚¹ï¼ˆsetup-chromeã§æ³¨å…¥ï¼‰ã«å¯¾å¿œ
    chrome_path = os.getenv("CHROME_PATH")
    if chrome_path:
        options.binary_location = chrome_path
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1280,2000")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/124.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


# =========================
# Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ï¼ˆæ·»ä»˜ç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«èª¿æ•´ï¼‰
# =========================
def format_datetime(dt_obj: datetime) -> str:
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def get_yahoo_news(keyword: str) -> pd.DataFrame:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢çµæœã‚’å–å¾—ã—ã¦ DataFrame ã§è¿”ã™
    ã‚«ãƒ©ãƒ : ã‚¿ã‚¤ãƒˆãƒ«, URL, æŠ•ç¨¿æ—¥, å¼•ç”¨å…ƒ
    """
    driver = make_driver()
    search_url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(search_url)
    time.sleep(5)  # åˆæœŸæç”»å¾…ã¡ï¼ˆæ·»ä»˜ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«æº–æ‹ ï¼‰:contentReference[oaicite:5]{index=5}

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    # li.sc-1u4589e-0 ç³»ã®ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å„é …ç›®ã‚’æŠ½å‡ºï¼ˆæ·»ä»˜ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ã®é¸æŠãƒ­ã‚¸ãƒƒã‚¯ã‚’è¸è¥²ï¼‰:contentReference[oaicite:6]{index=6}
    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            # æŠ•ç¨¿æ—¥ã®æ­£è¦åŒ–ï¼ˆ"YYYY/MM/DD HH:MM" ã‚’å„ªå…ˆã€‚å¤±æ•—æ™‚ã¯åŸæ–‡æ®‹ã—ï¼‰
            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = format_datetime(dt)
                except Exception:
                    pub_date = ds

            # åª’ä½“åï¼ˆå‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¨å®šï¼‰
            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div"
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                txt = el.get_text(" ", strip=True)
                # æŠ•ç¨¿æ—¥ã®æ–‡å­—åˆ—ãªã©ã‚’é™¤å»ï¼ˆæ·»ä»˜ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«æº–æ‹ ï¼‰:contentReference[oaicite:7]{index=7}
                txt = re.sub(r"\d{4}/\d{1,2}/\d{1,2} \d{2}:\d{2}", "", txt)
                txt = re.sub(r"\([^)]+\)", "", txt)
                txt = txt.strip()
                if txt and not txt.isdigit():
                    source = txt
                    break

            if title and url:
                rows.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source or "Yahoo"})
        except Exception:
            continue

    df = pd.DataFrame(rows, columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])
    return df


# =========================
# Release(åŒæœˆã‚¿ã‚°)ã®æ—¢å­˜Excelã‚’å–å¾—ã—ã¦çµåˆ
# =========================
def try_download_existing_from_release(repo: str, tag: str, asset_name: str, token: str) -> pd.DataFrame:
    """
    æ—¢å­˜ã®æœˆæ¬¡Excelï¼ˆasset_nameï¼‰ã‚’ Release(tag) ã‹ã‚‰å–å¾—ã—ã¦DFã§è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºDFã€‚
    """
    if not token or not repo or not tag:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    base = "https://api.github.com"
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

    # tagã‹ã‚‰releaseã‚’å–å¾—
    r = requests.get(f"{base}/repos/{repo}/releases/tags/{tag}", headers=headers)
    if r.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])
    rel = r.json()

    # ã‚¢ã‚»ãƒƒãƒˆæ¢ç´¢
    target = None
    for a in rel.get("assets", []):
        if a.get("name") == asset_name:
            target = a
            break

    if not target:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¢ã‚»ãƒƒãƒˆURLã¯application/octet-streamã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆï¼‰
    asset_url = target.get("url")
    headers_dl = headers | {"Accept": "application/octet-stream"}
    dr = requests.get(asset_url, headers=headers_dl)
    if dr.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    # ãƒã‚¤ãƒˆâ†’Excelèª­ã¿è¾¼ã¿
    with io.BytesIO(dr.content) as bio:
        try:
            df = pd.read_excel(bio, sheet_name="news")
            # å‹/åˆ—åã®å®‰å…¨åŒ–
            df = df[["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"]].copy()
        except Exception:
            df = pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    return df


# =========================
# ãƒ¡ã‚¤ãƒ³ï¼šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—â†’çµåˆâ†’é‡è¤‡æ’é™¤â†’ä¿å­˜
# =========================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--keyword", type=str, default=None)
    args = parser.parse_args()

    keyword = args.keyword or os.getenv("NEWS_KEYWORD") or DEFAULT_KEYWORD
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")

    # 1) Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
    df_new = get_yahoo_news(keyword)

    # 2) æ—¢å­˜ã®æœˆæ¬¡Excelï¼ˆReleaseï¼‰ã‚’å–å¾—ã—ã¦çµåˆ
    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")  # ä¾‹: owner/repo (Actionså†…ã§è‡ªå‹•æ³¨å…¥)
    tag = f"news-{ym_tag()}"
    asset = monthly_excel_name()

    df_old = try_download_existing_from_release(repo, tag, asset, token)

    # 3) çµåˆ & URLã§é‡è¤‡æ’é™¤ï¼ˆæ–°ã—ã„æ–¹ã‚’å„ªå…ˆï¼‰
    df_all = pd.concat([df_old, df_new], ignore_index=True)
    if not df_all.empty:
        # æŠ•ç¨¿æ—¥ãŒåŒã˜ã§ã‚‚URLãŒã‚­ãƒ¼ã€‚URLæ¬ æã¯è½ã¨ã™
        df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="last")
        # ä¸¦ã³ï¼ˆæ–°ã—ã„æ–¹ãŒä¸Šï¼‰: æŠ•ç¨¿æ—¥ã‚’æ–‡å­—åˆ—ã‹ã‚‰ä¸¦ã¹æ›¿ãˆã€‚å¤±æ•—æ™‚ã¯ãã®ã¾ã¾ã€‚
        try:
            dt = pd.to_datetime(df_all["æŠ•ç¨¿æ—¥"], errors="coerce", format="%Y/%m/%d %H:%M")
            df_all = df_all.assign(_dt=dt).sort_values("_dt", ascending=False).drop(columns=["_dt"])
        except Exception:
            pass

    # 4) ä¿å­˜ï¼ˆå˜ä¸€ã‚·ãƒ¼ãƒˆ newsï¼‰
    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", monthly_excel_name())
    with pd.ExcelWriter(out_path, engine="openpyxl") as w:
        df_all.to_excel(w, index=False, sheet_name="news")

    print(f"âœ… Excelå‡ºåŠ›: {out_path}ï¼ˆ{len(df_all)}ä»¶ã€ã†ã¡æ–°è¦ {len(df_new)}ä»¶ï¼‰")


if __name__ == "__main__":
    main()
